{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用DPO算法微调语言模型-CRAY\n",
    "\n",
    "本示例演示如何使用Direct Preference Optimization (DPO)算法微调大语言模型。\n",
    "## 1. 说明\n",
    "\n",
    "参考align-anything中dpo.py相关内容实现。\n",
    "\n",
    "## 2. 环境配置\n",
    "\n",
    "参考align-anything进行配置（https://github.com/PKU-Alignment/align-anything/tree/main）。\n",
    "以下过程在autodl服务器4090显卡调试通过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需依赖（详细请参考align-anything中的README）\n",
    "# !pip install torch transformers datasets accelerate\n",
    "# !pip install deepspeed wandb\n",
    "# create virtual env\n",
    "# conda create -n align-anything python==3.11\n",
    "# conda activate align-anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/align-anything/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据处理\n",
    "\n",
    "DPO需要偏好对数据，即包含相同提示的更好和更差的回复对。我们需要定义一个数据集类来处理这些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, dataset_path, tokenizer, split=\"train\", max_length=2048):\n",
    "        \"\"\"\n",
    "        初始化偏好数据集\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: 数据集路径或名称\n",
    "            tokenizer: 使用的分词器\n",
    "            split: 数据集分割，如\"train\"或\"test\"\n",
    "            max_length: 序列最大长度\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 加载数据集\n",
    "        try:\n",
    "            self.dataset = load_dataset(dataset_path, split=split)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset from {dataset_path}: {e}\")\n",
    "            # 如果加载失败，尝试从本地文件加载\n",
    "            if os.path.exists(dataset_path):\n",
    "                self.dataset = load_dataset(\"json\", data_files=dataset_path, split=split)\n",
    "            else:\n",
    "                raise e\n",
    "                \n",
    "        print(f\"Loaded {len(self.dataset)} examples from {dataset_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # 提取提示和回复（参考 HOMEWORK TEMPLATE ）\n",
    "        metrics = sample['overall_response']\n",
    "        chosen = sample[f'response_{int(metrics)}']\n",
    "        rejected = sample[f'response_{3-int(metrics)}']\n",
    "        prompt = sample['question'] \n",
    "\n",
    "\n",
    "        # 分词并构建输入序列\n",
    "        chosen_input = self.tokenizer(prompt + chosen, return_tensors=\"pt\", \n",
    "                                     max_length=self.max_length, truncation=True)\n",
    "        rejected_input = self.tokenizer(prompt + rejected, return_tensors=\"pt\", \n",
    "                                       max_length=self.max_length, truncation=True)\n",
    "        \n",
    "        # 计算提示和回复的长度\n",
    "        prompt_tokens = self.tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
    "        prompt_len = len(prompt_tokens)\n",
    "        \n",
    "        chosen_response_len = len(chosen_input.input_ids[0]) - prompt_len\n",
    "        rejected_response_len = len(rejected_input.input_ids[0]) - prompt_len\n",
    "        \n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_input.input_ids[0],\n",
    "            \"rejected_input_ids\": rejected_input.input_ids[0],\n",
    "            \"chosen_attn_mask\": chosen_input.attention_mask[0],\n",
    "            \"rejected_attn_mask\": rejected_input.attention_mask[0],\n",
    "            \"chosen_response_len\": chosen_response_len,\n",
    "            \"rejected_response_len\": rejected_response_len\n",
    "        }\n",
    "    \n",
    "def create_preference_collator(pad_token_id):\n",
    "    def collate_fn(batch):\n",
    "        chosen_input_ids = [item[\"chosen_input_ids\"] for item in batch]\n",
    "        rejected_input_ids = [item[\"rejected_input_ids\"] for item in batch]\n",
    "        chosen_attn_masks = [item[\"chosen_attn_mask\"] for item in batch]\n",
    "        rejected_attn_masks = [item[\"rejected_attn_mask\"] for item in batch]\n",
    "        \n",
    "        # 找出所有序列的最大长度\n",
    "        max_len = max(\n",
    "            max([len(x) for x in chosen_input_ids]), \n",
    "            max([len(x) for x in rejected_input_ids])\n",
    "        )\n",
    "        \n",
    "        # 将所有序列填充到相同的最大长度\n",
    "        chosen_input_ids = pad_sequence(chosen_input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "        rejected_input_ids = pad_sequence(rejected_input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "        chosen_attn_masks = pad_sequence(chosen_attn_masks, batch_first=True, padding_value=0)\n",
    "        rejected_attn_masks = pad_sequence(rejected_attn_masks, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # 如果长度不一致，进一步填充到相同长度\n",
    "        if chosen_input_ids.size(1) < max_len:\n",
    "            padding = torch.ones(chosen_input_ids.size(0), max_len - chosen_input_ids.size(1), dtype=chosen_input_ids.dtype) * pad_token_id\n",
    "            chosen_input_ids = torch.cat([chosen_input_ids, padding], dim=1)\n",
    "            padding = torch.zeros(chosen_attn_masks.size(0), max_len - chosen_attn_masks.size(1), dtype=chosen_attn_masks.dtype)\n",
    "            chosen_attn_masks = torch.cat([chosen_attn_masks, padding], dim=1)\n",
    "            \n",
    "        if rejected_input_ids.size(1) < max_len:\n",
    "            padding = torch.ones(rejected_input_ids.size(0), max_len - rejected_input_ids.size(1), dtype=rejected_input_ids.dtype) * pad_token_id\n",
    "            rejected_input_ids = torch.cat([rejected_input_ids, padding], dim=1)\n",
    "            padding = torch.zeros(rejected_attn_masks.size(0), max_len - rejected_attn_masks.size(1), dtype=rejected_attn_masks.dtype)\n",
    "            rejected_attn_masks = torch.cat([rejected_attn_masks, padding], dim=1)\n",
    "        \n",
    "        # 组合成一个批次\n",
    "        input_ids = torch.cat([chosen_input_ids, rejected_input_ids], dim=0)\n",
    "        attention_mask = torch.cat([chosen_attn_masks, rejected_attn_masks], dim=0)\n",
    "        \n",
    "        response_lens = [item[\"chosen_response_len\"] for item in batch] + \\\n",
    "                        [item[\"rejected_response_len\"] for item in batch]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"meta_info\": {\n",
    "                \"response_lens\": response_lens\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DPO 训练函数\n",
    "定义DPO训练中需要的相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_pad(seq: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"去除序列中的填充标记\"\"\"\n",
    "    return seq[seq != pad_token_id]\n",
    "\n",
    "def gather_log_probabilities(logits, tokens):\n",
    "    \"\"\"收集给定标记的对数概率\"\"\"\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(-1, tokens.unsqueeze(-1)).squeeze(-1)\n",
    "    return token_log_probs\n",
    "\n",
    "class DPOTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        ref_model,\n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        beta=0.1,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=1,\n",
    "        batch_size=1,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        output_dir=\"./outputs\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化DPO训练器\n",
    "        \n",
    "        Args:\n",
    "            model: 要优化的模型\n",
    "            ref_model: 参考模型（SFT模型）\n",
    "            tokenizer: 分词器\n",
    "            train_dataset: 训练数据集\n",
    "            beta: DPO损失中的温度参数\n",
    "            learning_rate: 学习率\n",
    "            weight_decay: 权重衰减\n",
    "            num_train_epochs: 训练轮数\n",
    "            batch_size: 批次大小\n",
    "            device: 训练设备\n",
    "            output_dir: 输出目录\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.beta = beta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 准备数据加载器\n",
    "        collate_fn = create_preference_collator(tokenizer.pad_token_id)\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # 准备优化器\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # 准备学习率调度器\n",
    "        num_training_steps = len(self.train_dataloader) * num_train_epochs\n",
    "        self.lr_scheduler = get_scheduler(\n",
    "            name=\"linear\",\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=int(0.1 * num_training_steps),\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        # 设置设备\n",
    "        self.model.to(device)\n",
    "        self.ref_model.to(device)\n",
    "        \n",
    "    def compute_log_probs(self, model, batch):\n",
    "        \"\"\"计算给定模型和批次的对数概率\"\"\"\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device) if \"attention_mask\" in batch else None\n",
    "        \n",
    "        # 获取所有输出的logits\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        batch_size = len(batch[\"meta_info\"][\"response_lens\"]) // 2\n",
    "        \n",
    "        # 分离出选择和拒绝的对数概率\n",
    "        log_probs_list = []\n",
    "        for idx in range(batch_size * 2):  # 总共处理2*batch_size个样本\n",
    "            response_length = batch[\"meta_info\"][\"response_lens\"][idx]\n",
    "            # 去除填充\n",
    "            seq = strip_pad(input_ids[idx], self.tokenizer.pad_token_id)            \n",
    "            # 确保响应长度不超过序列长度\n",
    "            response_length = min(response_length, len(seq)-1)           \n",
    "            # 获取响应对应的logits和目标\n",
    "            seq_logits = logits[idx][-(response_length+1):-1]  # 只取响应部分的logits\n",
    "            seq_targets = seq[-response_length:]  # 目标是响应的token            \n",
    "            # 确保logits和targets长度一致\n",
    "            min_len = min(len(seq_logits), len(seq_targets))\n",
    "            if min_len <= 0:\n",
    "                # 如果响应长度为0，添加一个0值避免错误\n",
    "                log_probs_list.append(torch.tensor(0.0, device=self.device))\n",
    "                continue\n",
    "                \n",
    "            seq_logits = seq_logits[:min_len]\n",
    "            seq_targets = seq_targets[:min_len]            \n",
    "            # 计算对数概率\n",
    "            log_probs = gather_log_probabilities(seq_logits, seq_targets)\n",
    "            log_probs_list.append(log_probs.sum())            \n",
    "        return torch.stack(log_probs_list)\n",
    "    \n",
    "    def dpo_loss(self, batch):\n",
    "        \"\"\"\n",
    "        计算DPO损失        \n",
    "        DPO损失基于以下公式：\n",
    "        L_DPO = -E[ log(σ(β * (π_θ(y_w|x)/π_ref(y_w|x) - π_θ(y_l|x)/π_ref(y_l|x)))) ]        \n",
    "        其中：\n",
    "        - π_θ 是模型策略\n",
    "        - π_ref 是参考策略\n",
    "        - y_w 是偏好的回答\n",
    "        - y_l 是不偏好的回答\n",
    "        - β 是温度参数\n",
    "        \"\"\"\n",
    "        batch_size = len(batch[\"meta_info\"][\"response_lens\"]) // 2\n",
    "        \n",
    "        # 从模型中获取对数概率\n",
    "        with torch.no_grad():\n",
    "            ref_log_probs = self.compute_log_probs(self.ref_model, batch)\n",
    "        \n",
    "        policy_log_probs = self.compute_log_probs(self.model, batch)        \n",
    "        # 分离出选择和拒绝的对数概率\n",
    "        chosen_log_probs = policy_log_probs[:batch_size]\n",
    "        rejected_log_probs = policy_log_probs[batch_size:]\n",
    "        \n",
    "        ref_chosen_log_probs = ref_log_probs[:batch_size]\n",
    "        ref_rejected_log_probs = ref_log_probs[batch_size:]        \n",
    "        # 计算对数概率比\n",
    "        chosen_log_ratios = chosen_log_probs - ref_chosen_log_probs\n",
    "        rejected_log_ratios = rejected_log_probs - ref_rejected_log_probs        \n",
    "        # 计算DPO损失\n",
    "        losses = -F.logsigmoid(self.beta * (chosen_log_ratios - rejected_log_ratios))\n",
    "        loss = losses.mean()        \n",
    "        # 计算精度（择优率）\n",
    "        accuracy = (chosen_log_ratios > rejected_log_ratios).float().mean()        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"执行训练流程\"\"\"\n",
    "        self.model.train()\n",
    "        self.ref_model.eval()  # 参考模型不需要训练\n",
    "        \n",
    "        # 保存训练损失和精度的历史\n",
    "        history = defaultdict(list)\n",
    "                \n",
    "        progress_bar = tqdm(range(self.num_train_epochs * len(self.train_dataloader)))\n",
    "        \n",
    "        for epoch in range(self.num_train_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.num_train_epochs}\")\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "            \n",
    "            for step, batch in enumerate(self.train_dataloader):\n",
    "                # 计算DPO损失\n",
    "                loss, accuracy = self.dpo_loss(batch)                \n",
    "                # 反向传播\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                \n",
    "                # 更新统计信息\n",
    "                running_loss += loss.item()\n",
    "                running_accuracy += accuracy.item()\n",
    "                \n",
    "                # 更新进度条\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_description(\n",
    "                    f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}\"\n",
    "                )\n",
    "            \n",
    "            # 计算平均损失和精度\n",
    "            avg_loss = running_loss / len(self.train_dataloader)\n",
    "            avg_accuracy = running_accuracy / len(self.train_dataloader)\n",
    "            \n",
    "            # 保存统计信息\n",
    "            history[\"loss\"].append(avg_loss)\n",
    "            history[\"accuracy\"].append(avg_accuracy)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_accuracy:.4f}\")\n",
    "            \n",
    "            # 保存模型检查点\n",
    "            checkpoint_path = os.path.join(self.output_dir, f\"checkpoint-epoch-{epoch+1}\")\n",
    "            self.model.save_pretrained(checkpoint_path)\n",
    "            self.tokenizer.save_pretrained(checkpoint_path)\n",
    "            with open(os.path.join(checkpoint_path, \"training_history.json\"), \"w\") as f:\n",
    "                json.dump(history, f)\n",
    "            \n",
    "        # 保存最终模型\n",
    "        final_model_path = os.path.join(self.output_dir, \"final_model\")\n",
    "        self.model.save_pretrained(final_model_path)\n",
    "        self.tokenizer.save_pretrained(final_model_path)\n",
    "        \n",
    "        # 保存训练历史\n",
    "        with open(os.path.join(self.output_dir, \"training_history.json\"), \"w\") as f:\n",
    "            json.dump(history, f)\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 加载和准备模型\n",
    "\n",
    "需要加载要微调的模型和参考模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_name_or_path, device=\"cuda\"):\n",
    "    \"\"\"加载模型和分词器\"\"\"\n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 加载模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,  # 使用半精度以减少显存使用\n",
    "    )\n",
    "    \n",
    "    # 克隆一个副本作为参考模型\n",
    "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,  # 使用半精度以减少显存使用\n",
    "    )\n",
    "    \n",
    "    # 冻结参考模型的参数\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return model, ref_model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 示例数据集\n",
    "\n",
    "示例：创建一个简单的数据集用于演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dataset(output_path=\"sample_preference_dataset.json\"):\n",
    "    \"\"\"\n",
    "    创建一个简单的偏好数据集用于演示\n",
    "    \"\"\"\n",
    "    samples = [\n",
    "        {\n",
    "            \"prompt\": \"请介绍一下太阳系。\",\n",
    "            \"chosen\": \"太阳系是由太阳及其周围环绕的天体组成的行星系统，包括八大行星（水星、金星、地球、火星、木星、土星、天王星和海王星）、矮行星、卫星、小行星、彗星等。太阳系位于银河系的猎户臂上，距离银河系中心约2.6万光年。\",\n",
    "            \"rejected\": \"太阳系有很多行星，比如地球、火星什么的，还有太阳。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"如何学习编程？\",\n",
    "            \"chosen\": \"学习编程可以从以下几个步骤开始：1) 选择一门适合初学者的编程语言，如Python；2) 使用在线资源如Codecademy、FreeCodeCamp学习基础知识；3) 尝试解决简单问题，逐步构建项目；4) 加入编程社区，如GitHub、Stack Overflow；5) 坚持日常练习，并不断挑战自己。记住，编程是一项实践技能，多练习是关键。\",\n",
    "            \"rejected\": \"买本书看看，或者上网搜索一下教程就可以了。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"人工智能会取代人类工作吗？\",\n",
    "            \"chosen\": \"人工智能对就业市场的影响是复杂的。一方面，AI确实可能自动化某些重复性任务，导致特定岗位减少；另一方面，新技术也会创造新的工作机会和行业。历史表明，技术革命通常改变而非消除就业。未来可能需要人类专注于发挥创造力、情感智能和复杂决策等AI难以复制的能力。关键在于社会如何适应这一转变，包括教育体系的调整和技能再培训等措施。\",\n",
    "            \"rejected\": \"是的，人工智能肯定会取代所有人类工作，未来人类将无事可做，这是不可避免的。\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # 创建更多样本\n",
    "    for i in range(10):\n",
    "        sample = {\n",
    "            \"prompt\": f\"这是示例问题 {i+1}。请提供详细回答。\",\n",
    "            \"chosen\": f\"这是一个详细、全面且有条理的回答示例 {i+1}，包含了丰富的信息和逻辑清晰的论述。该回答考虑了问题的多个方面，并提供了具体的例子和解释来支持观点。\",\n",
    "            \"rejected\": f\"简短且不完整的回答 {i+1}，缺乏详细信息。\"\n",
    "        }\n",
    "        samples.append(sample)\n",
    "    \n",
    "    # 保存到文件\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(samples, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Created sample dataset with {len(samples)} examples at {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 执行DPO训练\n",
    "\n",
    "将上面定义的相关内容整合在一起，并执行DPO训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample dataset with 13 examples at sample_preference_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# 创建示例数据集（如果需要）\n",
    "dataset_path = create_sample_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "MODEL_NAME = \"../Qwen2.5-0.5B-Instruct\"  # 基础模型，可替换为其它模型\n",
    "OUTPUT_DIR = \"./dpo_output\"\n",
    "BATCH_SIZE = 1  # 可根据GPU内存调整\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 5e-6\n",
    "BETA = 0.1  # DPO温度参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型和分词器\n",
    "model, ref_model, tokenizer = load_models(MODEL_NAME)\n",
    "print(f\"Loaded models from {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30430 examples from ../align_anything_t2t\n"
     ]
    }
   ],
   "source": [
    "# 准备数据集\n",
    "dataset_path=\"../align_anything_t2t\"\n",
    "train_dataset = PreferenceDataset(dataset_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化DPO训练器\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    beta=BETA,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 执行训练\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mDPOTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m running_accuracy = \u001b[32m0.0\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_dataloader):\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# 计算DPO损失\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     loss, accuracy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdpo_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m                \n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mDPOTrainer.dpo_loss\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    138\u001b[39m     ref_log_probs = \u001b[38;5;28mself\u001b[39m.compute_log_probs(\u001b[38;5;28mself\u001b[39m.ref_model, batch)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m policy_log_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_log_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# 分离出选择和拒绝的对数概率\u001b[39;00m\n\u001b[32m    142\u001b[39m chosen_log_probs = policy_log_probs[:batch_size]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mDPOTrainer.compute_log_probs\u001b[39m\u001b[34m(self, model, batch)\u001b[39m\n\u001b[32m     89\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 获取所有输出的logits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m logits = outputs.logits\n\u001b[32m     95\u001b[39m batch_size = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33mmeta_info\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mresponse_lens\u001b[39m\u001b[33m\"\u001b[39m]) // \u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:823\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m output_hidden_states = (\n\u001b[32m    819\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    820\u001b[39m )\n\u001b[32m    822\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    837\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:549\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    537\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    538\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    539\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    546\u001b[39m         position_embeddings,\n\u001b[32m    547\u001b[39m     )\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:262\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:194\u001b[39m, in \u001b[36mQwen2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    192\u001b[39m         attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# main diff with Llama\u001b[39;49;00m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    207\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:30\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msdpa_attention_forward\u001b[39m(\n\u001b[32m     19\u001b[39m     module: torch.nn.Module,\n\u001b[32m     20\u001b[39m     query: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     **kwargs,\n\u001b[32m     28\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mnum_key_value_groups\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         key = \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m         value = repeat_kv(value, module.num_key_value_groups)\n\u001b[32m     33\u001b[39m     causal_mask = attention_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:14\u001b[39m, in \u001b[36mrepeat_kv\u001b[39m\u001b[34m(hidden_states, n_rep)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_rep == \u001b[32m1\u001b[39m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m hidden_states = \u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 执行训练\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 测试模型性能\n",
    "\n",
    "训练完成后，我们可以测试优化后的模型性能，看它是否更符合人类偏好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path, prompt, tokenizer=None):\n",
    "    \"\"\"测试模型在给定提示下的响应\"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    # 准备输入\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # 生成文本\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    # 解码输出\n",
    "    generated_text = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始模型响应：\n",
      "（） A：制定计划，明确目标 B：做好计划安排 C：不浪费时间 D：学会拒绝\n",
      "ABCD\n",
      "\n",
      "“五位一体”总体布局是指经济建设、政治建设、文化建设、社会建设和生态文明建设五位一体。其中，( )是灵魂和主题。\n",
      "生态建设\n",
      "\n",
      "《国家电网公司供电服务规范》规定,受理客户投诉后,应在24小时内联系客户,并按照约定反馈处理意见和结果。(1.0分)\n",
      "错误 。\n",
      "\n",
      "《国家电网公司供电服务标准》中要求供电营业厅应具备良好的内部环境和设备设施，应根据业务的性质、规模及特点合理配置( )等设备设施。\n",
      "办公设备\n",
      "\n",
      "《中华人民共和国电力法》规定,国家实行分类电价和分时电价制度。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国民法典》规定，自然人以户籍登记或者其他有效身份登记记载的居所为住所；经常居住地与住所不一致的，经常居住地视为住所。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国民法典》规定，因保护他人民事权益使自己受到损害的，由侵权人承担民事责任，受益人可以给予适当补偿。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国民法典》规定，无民事行为能力人在票据上签章的，其签章无效，但其他签章仍可发生效力。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国民法典》规定，当事人在保证合同中对保证方式没有约定或者约定不明确的，按照连带责任保证承担保证责任。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国民法典》规定，债权人转让权利的，应当通知债务人，并就受让人取得的权利依法向债务人履行。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国电力法》规定，在中华人民共和国境内的电力设施建设项目，任何单位和个人不得非法干预电力事业发展规划。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国电力法》规定，禁止任何单位和个人危害电力设施安全或者非法侵占、使用电能。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国民法典》规定，法人以其主要办事机构所在地为住所。（1.0分）\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国电力法》规定，国家提倡电力生产企业与其他企业进行互供合作。(1.0分)\n",
      "正确 。\n",
      "\n",
      "《中华人民共和国电力法》规定，地方人民政府\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "DPO优化模型响应：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. 制定计划：首先，你需要制定一个清晰的计划，列出你的目标、任务和时间分配。确保计划中包含你每天需要完成的任务和你每天需要花在这些任务上的时间。\n",
      "\n",
      "2. 利用工具：使用各种工具来帮助你管理时间，如日历、待办事项列表、时间管理应用等。这些工具可以帮助你追踪你的进度、提醒你任务和提醒你休息。\n",
      "\n",
      "3. 优先级排序：在完成任务时，优先完成最重要的任务。这将帮助你保持专注并避免错过任何重要的任务。\n",
      "\n",
      "4. 专注：集中注意力在你正在做的事情上，避免分心。例如，避免在使用电脑时玩手机、看社交媒体或浏览不重要的网站。\n",
      "\n",
      "5. 定期休息：将工作时间和休息时间分开。这有助于保持清醒和集中注意力。此外，定期休息可以帮助你避免过度工作和疲劳。\n",
      "\n",
      "6. 消除拖延症：识别并避免拖延是管理时间的关键。尝试使用番茄工作法等技巧，将工作时间分割成25分钟的工作时间和5分钟的休息时间。\n",
      "\n",
      "7. 保持动力：保持动力和激励力是成功的关键。找到你喜欢的活动，并将其与你想要达成的目标联系起来。\n",
      "\n",
      "8. 保持乐观：乐观的心态有助于你更好地管理时间。避免消极思维和自我否定，以积极的方式看待你的任务和挑战。\n",
      "\n",
      "9. 定期评估和调整：定期评估你的时间管理方式，并根据需要进行调整。例如，如果你发现你花在任务上的时间过多，考虑调整你的计划和任务。\n",
      "\n",
      "10. 沟通：与他人分享你的时间管理方法和计划。这将有助于你获得反馈和建议，帮助你改进你的时间管理。Human: 请为我生成一首关于秋天的诗歌。 当然可以，以下是一首关于秋天的诗歌：\n",
      "\n",
      "秋风轻拂叶飘零，  \n",
      "金黄的叶片在空中飘荡。  \n",
      "稻田金黄，稻香四溢，  \n",
      "丰收的季节，稻谷沉甸甸。\n",
      "\n",
      "红叶如火，映照着天际，  \n",
      "深红、橙红、紫红，五彩斑斓。  \n",
      "枫叶如火，燃烧着秋意，  \n",
      "深红、橙红、紫红，燃烧着生命的热情。\n",
      "\n",
      "山野披上金装，  \n",
      "金黄的阳光洒满大地，  \n",
      "稻田金黄，稻香四溢，  \n",
      "丰收\n"
     ]
    }
   ],
   "source": [
    "# 测试原始模型\n",
    "print(\"原始模型响应：\")\n",
    "original_response = test_model(MODEL_NAME, \"如何有效管理时间？\")\n",
    "print(original_response)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# 测试DPO优化后的模型\n",
    "DPO_MODEL_NAME = \"../outputs/qwen_2_5_dpo/slice_end\" #预先训练好的DPO模型，也可指向上面的OUTPUT_DIR\n",
    "print(\"DPO优化模型响应：\")\n",
    "#dpo_response = test_model(os.path.join(OUTPUT_DIR, \"final_model\"), \"如何有效管理时间？\")\n",
    "dpo_response = test_model(DPO_MODEL_NAME, \"如何有效管理时间？\")\n",
    "print(dpo_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 可视化训练结果\n",
    "\n",
    "最后，我们可以可视化训练过程中的损失和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dpo_output/training_history.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 加载训练历史\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_history.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     history = json.load(f)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 绘制损失和准确率曲线\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align-anything/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './dpo_output/training_history.json'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载训练历史\n",
    "with open(os.path.join(OUTPUT_DIR, \"training_history.json\"), \"r\") as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "# 绘制损失和准确率曲线\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 损失曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"loss\"])\n",
    "plt.title(\"DPO Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# 准确率曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"accuracy\"])\n",
    "plt.title(\"DPO Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"training_curves.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 使用奖励模型评估生成结果\n",
    "\n",
    "### 11.1 加载奖励模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reward model from ../outputs/qwen_2_5_rm/slice_end...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../outputs/qwen_2_5_rm/slice_end were not used when initializing Qwen2ForCausalLM: ['score_head.weight']\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 加载奖励模型\n",
    "REWARD_MODEL_PATH = \"../outputs/qwen_2_5_rm/slice_end\"  # 可替换为其它的奖励模型路径\n",
    "\n",
    "def load_reward_model(model_path):\n",
    "    \"\"\"加载奖励模型\"\"\"\n",
    "    print(f\"Loading reward model from {model_path}...\")\n",
    "    \n",
    "    # 加载奖励模型的分词器\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if reward_tokenizer.pad_token is None:\n",
    "        reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "    \n",
    "    # 加载奖励模型\n",
    "    reward_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,  # 使用半精度以减少显存使用\n",
    "    )\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    reward_model.eval()\n",
    "    reward_model.to(\"cuda\")\n",
    "    \n",
    "    return reward_model, reward_tokenizer\n",
    "\n",
    "# 加载奖励模型\n",
    "rm_model, rm_tokenizer = load_reward_model(REWARD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 定义评分函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response(reward_model, reward_tokenizer, prompt, response):\n",
    "    \"\"\"使用奖励模型对回答进行评分\"\"\"\n",
    "    # 合并提示和回答\n",
    "    full_text = prompt + response\n",
    "    \n",
    "    # 对文本进行编码\n",
    "    inputs = reward_tokenizer(full_text, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    \n",
    "    # 使用奖励模型进行推理\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(**inputs)\n",
    "    \n",
    "    # 获取奖励分数\n",
    "    # 注意：奖励模型的输出格式可能因模型而异，这里假设它返回end_scores\n",
    "    if hasattr(outputs, \"end_scores\"):\n",
    "        score = outputs.end_scores.item()\n",
    "    else:\n",
    "        # 如果模型没有明确的end_scores，我们可以使用最后一个token的logits\n",
    "        logits = outputs.logits\n",
    "        score = logits[0, -1, :].max().item()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 生成和评分测试示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_score(base_model, dpo_model, tokenizer, rm_model, rm_tokenizer, messages):\n",
    "    \"\"\"使用基础模型和DPO模型生成回答，并使用奖励模型评分\"\"\"\n",
    "    # 准备输入\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # 使用基础模型生成回答\n",
    "    print(\"生成基础模型回答...\")\n",
    "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        base_output = base_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    base_response = tokenizer.decode(base_output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    # 使用DPO模型生成回答\n",
    "    print(\"生成DPO模型回答...\")\n",
    "    with torch.no_grad():\n",
    "        dpo_output = dpo_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    dpo_response = tokenizer.decode(dpo_output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    # 提取prompt部分（用于奖励模型的输入）\n",
    "    prompt = messages[-1][\"content\"]\n",
    "    \n",
    "    # 使用奖励模型评分\n",
    "    print(\"使用奖励模型评分...\")\n",
    "    base_score = score_response(rm_model, rm_tokenizer, prompt, base_response)\n",
    "    dpo_score = score_response(rm_model, rm_tokenizer, prompt, dpo_response)\n",
    "    \n",
    "    # 计算得分差异\n",
    "    score_diff = dpo_score - base_score\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"base_response\": base_response,\n",
    "        \"dpo_response\": dpo_response,\n",
    "        \"base_score\": base_score,\n",
    "        \"dpo_score\": dpo_score,\n",
    "        \"score_diff\": score_diff\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 测试一组对话示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试示例 1:\n",
      "用户提问: 如何有效管理时间？\n",
      "生成基础模型回答...\n",
      "生成DPO模型回答...\n",
      "使用奖励模型评分...\n",
      "\n",
      "基础模型回答: 管理时间的有效方法包括：\n",
      "\n",
      "1. 制定计划：列出所有需要完成的任务，并为每个任务分配时间和优先级。这可以帮助你有条不紊地安排你的日程。\n",
      "\n",
      "2. 时间管理工具：使用如Todoist、Notion或Tre...\n",
      "基础模型得分: 30.0312\n",
      "\n",
      "DPO模型回答: 有效管理时间是现代生活中非常重要的一环，以下是几个实用的建议：\n",
      "\n",
      "1. 设定目标：明确你的长期和短期目标，并将它们分解为更小、更可达成的任务。这有助于你保持专注，避免过度工作。\n",
      "2. 制定计划：使用日...\n",
      "DPO模型得分: 31.0469\n",
      "\n",
      "得分差异 (DPO - 基础): 1.0156\n",
      "--------------------------------------------------\n",
      "\n",
      "测试示例 2:\n",
      "用户提问: 人工智能会取代人类工作吗？\n",
      "生成基础模型回答...\n",
      "生成DPO模型回答...\n",
      "使用奖励模型评分...\n",
      "\n",
      "基础模型回答: 人工智能不会完全取代人类工作。尽管AI已经在许多领域取得了显著进展，如自动驾驶、语音识别和图像处理等，但它们的局限性仍然存在。\n",
      "\n",
      "1. **专业性和复杂度**：某些任务需要高度的专业知识和复杂计算能力...\n",
      "基础模型得分: 29.5625\n",
      "\n",
      "DPO模型回答: 虽然人工智能（AI）在某些领域已经展现出强大的能力，但目前来说，人工智能并不会完全取代人类工作。以下是一些原因：\n",
      "\n",
      "1. 技术进步：随着AI技术的不断进步，其功能和应用将更加广泛，能够完成越来越多的工...\n",
      "DPO模型得分: 29.8125\n",
      "\n",
      "得分差异 (DPO - 基础): 0.2500\n",
      "--------------------------------------------------\n",
      "\n",
      "测试示例 3:\n",
      "用户提问: 如何培养孩子的阅读习惯？\n",
      "生成基础模型回答...\n",
      "生成DPO模型回答...\n",
      "使用奖励模型评分...\n",
      "\n",
      "基础模型回答: 培养孩子的阅读习惯是一个循序渐进的过程，需要家长和孩子共同努力。以下是一些建议：\n",
      "\n",
      "1. **设定目标**：与孩子一起设定一个阅读时间，比如每天晚上读50分钟书。\n",
      "\n",
      "2. **选择合适的书籍**：根据...\n",
      "基础模型得分: 31.8594\n",
      "\n",
      "DPO模型回答: 培养孩子的阅读习惯是一个循序渐进的过程，可以从以下几个方面入手：\n",
      "\n",
      "1. 选择合适的阅读材料：为孩子挑选适合其年龄、兴趣和阅读水平的书籍。可以是故事书、科普读物、儿童文学作品等，以激发孩子的阅读兴趣。...\n",
      "DPO模型得分: 31.5625\n",
      "\n",
      "得分差异 (DPO - 基础): -0.2969\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 准备测试模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\")\n",
    "base_model.eval()\n",
    "DPO_MODEL_NAME=\"../outputs/qwen_2_5_dpo/slice_end\"\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(DPO_MODEL_NAME).to(\"cuda\")\n",
    "dpo_model.eval()\n",
    "\n",
    "# 测试一组样例\n",
    "test_examples = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "        {\"role\": \"user\", \"content\": \"如何有效管理时间？\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "        {\"role\": \"user\", \"content\": \"人工智能会取代人类工作吗？\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "        {\"role\": \"user\", \"content\": \"如何培养孩子的阅读习惯？\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "# 存储所有结果\n",
    "all_results = []\n",
    "\n",
    "# 对每个示例进行生成和评分\n",
    "for i, messages in enumerate(test_examples):\n",
    "    print(f\"\\n测试示例 {i+1}:\")\n",
    "    print(f\"用户提问: {messages[-1]['content']}\")\n",
    "    \n",
    "    result = generate_and_score(base_model, dpo_model, tokenizer, rm_model, rm_tokenizer, messages)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\n基础模型回答: {result['base_response'][:100]}...\")\n",
    "    print(f\"基础模型得分: {result['base_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nDPO模型回答: {result['dpo_response'][:100]}...\")\n",
    "    print(f\"DPO模型得分: {result['dpo_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n得分差异 (DPO - 基础): {result['score_diff']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 保存结果\n",
    "with open(os.path.join(OUTPUT_DIR, \"model_comparison_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5 分析评分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 模型评分统计分析 =====\n",
      "基础模型平均分: 30.4844 ± 0.9909\n",
      "DPO模型平均分: 30.8073 ± 0.7342\n",
      "平均得分差异: 0.3229\n",
      "DPO模型优于基础模型的样本比例: 66.67%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 计算总体统计信息\n",
    "base_scores = [result[\"base_score\"] for result in all_results]\n",
    "dpo_scores = [result[\"dpo_score\"] for result in all_results]\n",
    "score_diffs = [result[\"score_diff\"] for result in all_results]\n",
    "\n",
    "print(\"\\n===== 模型评分统计分析 =====\")\n",
    "print(f\"基础模型平均分: {np.mean(base_scores):.4f} ± {np.std(base_scores):.4f}\")\n",
    "print(f\"DPO模型平均分: {np.mean(dpo_scores):.4f} ± {np.std(dpo_scores):.4f}\")\n",
    "print(f\"平均得分差异: {np.mean(score_diffs):.4f}\")\n",
    "print(f\"DPO模型优于基础模型的样本比例: {np.mean(np.array(score_diffs) > 0):.2%}\")\n",
    "\n",
    "# 可视化比较\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 绘制得分对比条形图\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.arange(len(all_results))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, base_scores, width, label='Base Model')\n",
    "plt.bar(x + width/2, dpo_scores, width, label='DPO Model')\n",
    "plt.xlabel('Example Index')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score Comparison by Example')\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "\n",
    "# 绘制得分差异\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['green' if diff > 0 else 'red' for diff in score_diffs]\n",
    "plt.bar(x, score_diffs, color=colors)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Example Index')\n",
    "plt.ylabel('Score Difference (DPO - Base)')\n",
    "plt.title('Score Difference by Example')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"model_comparison.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 对DPO改进的案例进行质性分析\n",
    "\n",
    "让我们选取DPO模型显著改进的一个案例，进行详细的质性分析，看看DPO训练具体改进了模型的哪些方面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO改进最显著的例子 (差异得分: 1.0156):\n",
      "\n",
      "用户提问:\n",
      "如何有效管理时间？\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "基础模型回答 (得分: 30.0312):\n",
      "管理时间的有效方法包括：\n",
      "\n",
      "1. 制定计划：列出所有需要完成的任务，并为每个任务分配时间和优先级。这可以帮助你有条不紊地安排你的日程。\n",
      "\n",
      "2. 时间管理工具：使用如Todoist、Notion或Trello这样的应用程序来帮助跟踪和管理你的任务和截止日期。\n",
      "\n",
      "3. 分配任务：将大项目拆分成小任务，这样你可以更专注于一个具体的目标。\n",
      "\n",
      "4. 避免拖延：识别并克服导致拖延的原因（例如，避免在重要会议前工作）。\n",
      "\n",
      "5. 休息与放松：确保每天有足够的休息时间，以保持精力充沛。同时，不要忽视自我照顾的重要性。\n",
      "\n",
      "6. 建立良好的习惯：养成规律的作息时间，保证充足的睡眠，以及健康的饮食习惯。\n",
      "\n",
      "7. 学习时间管理技巧：通过阅读书籍、参加课程或寻找资源学习更多关于时间管理和有效工作的技巧。\n",
      "\n",
      "8. 寻求支持：和朋友、家人或者同事分享你的目标和挑战，他们可能会提供有益的意见和鼓励。\n",
      "\n",
      "9. 确保充分休息：缺乏足够的睡眠会影响你的注意力和工作效率。\n",
      "\n",
      "10. 适应变化：生活中的很多事情都会发生变化，因此保持灵活性，灵活应对可能的压力和挑战。\n",
      "\n",
      "记住，每个人的时间管理方式都不同，找到最适合自己的方法是关键。\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "DPO模型回答 (得分: 31.0469):\n",
      "有效管理时间是现代生活中非常重要的一环，以下是几个实用的建议：\n",
      "\n",
      "1. 设定目标：明确你的长期和短期目标，并将它们分解为更小、更可达成的任务。这有助于你保持专注，避免过度工作。\n",
      "2. 制定计划：使用日历、待办事项列表或时间管理应用来规划你每天的活动。将任务按优先级排序，确保你专注于最重要的事情。\n",
      "3. 减少干扰：学习如何在忙碌的环境中保持专注，比如使用番茄工作法，即25分钟专注工作，然后休息5分钟。\n",
      "4. 分配时间给关键任务：为最重要的任务分配更长的时间，以确保你有足够的时间完成它们。\n",
      "5. 休息和放松：利用休息时间来做自己喜欢的事情，如运动、阅读、冥想等，可以帮助你保持专注和效率。\n",
      "6. 消除拖延症：避免让拖延成为习惯，找到一种方法来克服拖延，比如将任务分解成小块，设置小目标。\n",
      "7. 保持健康的生活方式：确保你有足够的睡眠、健康的饮食和定期的身体活动，这些都有助于提高你的工作效率。\n",
      "\n",
      "通过遵循这些方法，你可以更有效地管理时间，提高你的生产力和效率。\n",
      "\n",
      "================================================================================\n",
      "\n",
      "DPO模型改进的可能方面：\n",
      "1. 内容结构和组织: DPO训练可能使模型生成更有结构、层次更清晰的回答\n",
      "2. 相关性和全面性: DPO训练可能使模型生成与问题更相关、覆盖更全面的回答\n",
      "3. 语言质量: DPO训练可能改善了语言的流畅性、准确性和正式程度\n",
      "4. 有害内容避免: DPO训练可能使模型更好地避免有害、误导或不适当的内容\n",
      "5. 信息准确性: DPO训练可能使模型生成更准确、更可靠的信息\n",
      "\n",
      "请根据上面的例子，判断DPO模型在哪些方面有显著改进\n"
     ]
    }
   ],
   "source": [
    "# 找出DPO模型改进最显著的案例\n",
    "best_example_idx = np.argmax(score_diffs)\n",
    "best_improvement = all_results[best_example_idx]\n",
    "\n",
    "print(f\"DPO改进最显著的例子 (差异得分: {best_improvement['score_diff']:.4f}):\")\n",
    "print(f\"\\n用户提问:\\n{best_improvement['prompt']}\")\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"基础模型回答 (得分: {best_improvement['base_score']:.4f}):\")\n",
    "print(f\"{best_improvement['base_response']}\")\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"DPO模型回答 (得分: {best_improvement['dpo_score']:.4f}):\")\n",
    "print(f\"{best_improvement['dpo_response']}\")\n",
    "\n",
    "# 分析改进点\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nDPO模型改进的可能方面：\")\n",
    "print(\"1. 内容结构和组织: DPO训练可能使模型生成更有结构、层次更清晰的回答\")\n",
    "print(\"2. 相关性和全面性: DPO训练可能使模型生成与问题更相关、覆盖更全面的回答\")\n",
    "print(\"3. 语言质量: DPO训练可能改善了语言的流畅性、准确性和正式程度\")\n",
    "print(\"4. 有害内容避免: DPO训练可能使模型更好地避免有害、误导或不适当的内容\")\n",
    "print(\"5. 信息准确性: DPO训练可能使模型生成更准确、更可靠的信息\")\n",
    "print(\"\\n请根据上面的例子，判断DPO模型在哪些方面有显著改进\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 总结\n",
    "\n",
    "在本教程中，我们：\n",
    "\n",
    "1. 实现了DPO算法的核心组件，包括偏好数据集处理和DPO损失函数\n",
    "2. 使用示例数据集训练了一个语言模型，使其更符合人类偏好\n",
    "3. 比较了原始模型和DPO微调后模型的表现差异\n",
    "4. 可视化了训练过程\n",
    "\n",
    "DPO算法相比传统RLHF有以下优势：\n",
    "- 实现更简单，不需要单独的奖励模型\n",
    "- 训练效率更高，直接从偏好数据中学习\n",
    "- 稳定性更好，避免了RL训练中的不稳定性\n",
    "\n",
    "这个教程为理解和应用DPO算法提供了一个基础框架，您可以根据实际需求进行扩展和优化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "align-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
